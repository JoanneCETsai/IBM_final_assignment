{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cuisine</th>\n",
       "      <th>ingredients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22675</td>\n",
       "      <td>italian</td>\n",
       "      <td>[1% low-fat cottage cheese, low-fat marinara s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32288</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[brown sugar, salt, eggs, butter, chopped peca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44406</td>\n",
       "      <td>thai</td>\n",
       "      <td>[red chili peppers, bell pepper, garlic, fish ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29355</td>\n",
       "      <td>moroccan</td>\n",
       "      <td>[water, green tea leaves, tangerine, fresh min...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39350</td>\n",
       "      <td>chinese</td>\n",
       "      <td>[vegetable oil, chile sauce, tomato paste, gar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29769</th>\n",
       "      <td>2278</td>\n",
       "      <td>japanese</td>\n",
       "      <td>[soy sauce, sesame oil, garlic, sake, flour, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29770</th>\n",
       "      <td>474</td>\n",
       "      <td>vietnamese</td>\n",
       "      <td>[mint, garlic sauce, chinese chives, rice nood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29771</th>\n",
       "      <td>44229</td>\n",
       "      <td>indian</td>\n",
       "      <td>[potatoes, vegetable broth, oil, cashew nuts, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29772</th>\n",
       "      <td>20311</td>\n",
       "      <td>southern_us</td>\n",
       "      <td>[butter, powdered sugar, cream cheese, soften,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29773</th>\n",
       "      <td>32823</td>\n",
       "      <td>chinese</td>\n",
       "      <td>[savoy cabbage, dumpling wrappers, ginger, soy...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29774 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      cuisine                                        ingredients\n",
       "0      22675      italian  [1% low-fat cottage cheese, low-fat marinara s...\n",
       "1      32288  southern_us  [brown sugar, salt, eggs, butter, chopped peca...\n",
       "2      44406         thai  [red chili peppers, bell pepper, garlic, fish ...\n",
       "3      29355     moroccan  [water, green tea leaves, tangerine, fresh min...\n",
       "4      39350      chinese  [vegetable oil, chile sauce, tomato paste, gar...\n",
       "...      ...          ...                                                ...\n",
       "29769   2278     japanese  [soy sauce, sesame oil, garlic, sake, flour, g...\n",
       "29770    474   vietnamese  [mint, garlic sauce, chinese chives, rice nood...\n",
       "29771  44229       indian  [potatoes, vegetable broth, oil, cashew nuts, ...\n",
       "29772  20311  southern_us  [butter, powdered sugar, cream cheese, soften,...\n",
       "29773  32823      chinese  [savoy cabbage, dumpling wrappers, ginger, soy...\n",
       "\n",
       "[29774 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib3\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "df=pd.read_json('train.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6231"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "features_all_list=[]\n",
    "for i in df.ingredients:\n",
    "    features_all_list+=i\n",
    "\n",
    "features=list(set(features_all_list))\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_ingredients = np.zeros((df.shape[0], len(features)))\n",
    "feature_lookup = sorted(features)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    for ingredient in row['ingredients']:\n",
    "        onehot_ingredients[index, feature_lookup.index(ingredient)] = 1\n",
    "y = df.cuisine.values.reshape(-1,1)\n",
    "y=y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29774, 6231)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.DataFrame(onehot_ingredients)\n",
    "d = {}\n",
    "for i in range(len(features)):\n",
    "    d[df_features.columns[i]] = features[i]\n",
    "    \n",
    "df_features = df_features.rename(columns=d)\n",
    "df_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_features, y, test_size=0.3, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_1: solver用'adam'，官方建議在資料集較大的時候用'adam'這個優化方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, random_state=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1=MLPClassifier(solver='adam', alpha=1e-5,hidden_layer_sizes=(100, ), random_state=1)\n",
    "model_1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_2: solver用'lbfgs'，官方建議在資料集較小的時候用'lbfgs'這個優化方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=1e-05, random_state=1, solver='lbfgs')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2=MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(100, ), random_state=1)\n",
    "model_2.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_1 (solver='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1    2    3   4    5    6    7   8     9   10   11   12    13   14  \\\n",
      "0   53   0    3    1   4    4    0    1   1     6   1    0    0    21    1   \n",
      "1    1  73    1    1   1   24    3    3  20    17   0    2    0     3    0   \n",
      "2    1   3  212    2   1   14    1    0   4    13   1    1    0    17    0   \n",
      "3    1   1    4  462  10    5    1    6   0     4   0   29   22     8    1   \n",
      "4    5   0    1   18  86    1    0    3   1     9   0    6    3    12    0   \n",
      "5    1  11    9    7   3  305   15    7  10   131   2    1    2    18    3   \n",
      "6    0   0    1    4   1    7  164    6   2    36   0    0    1     6    4   \n",
      "7    2   3    2    4   4    8    7  574   1     5   1   20    0    12   15   \n",
      "8    0  11    2    0   3    9    3    1  45     9   1    0    0     3    0   \n",
      "9    1   6    9    7   4  124   44    5  10  1422   4    1    0    30    5   \n",
      "10   5   2    1    1   3    1    2    5   0     4  74    0    0    16    0   \n",
      "11   0   0    3   33   4   14    0   14   1     6   0  204   10     5    0   \n",
      "12   0   0    0   34   0    3    0    0   0     3   0   13  123     2    0   \n",
      "13   6   5    9    8   4   18    7   20   0    26   0    5    0  1252    3   \n",
      "14   1   1    2    0   1    4    5   16   0     9   0    0    0     6  148   \n",
      "15   0   5    0    1   6   13    8    2   6    14   0    1    0     3    0   \n",
      "16   6  18   53    6   7   40    6   10  22    46   8    6    0    44    6   \n",
      "17   2   4    2    1   1   24    5    0   1    50   1    4    0    35    1   \n",
      "18   4   0    1   20   3    1    0   16   0     5   2    8    1     2    1   \n",
      "19   0   0    2   29   8    0    1    5   0     2   1    9    3     1    0   \n",
      "\n",
      "    15   16  17   18  19  \n",
      "0    1    6  11    4   0  \n",
      "1    3   25   4    0   0  \n",
      "2    3   67   9    0   0  \n",
      "3    2   10   2   17  24  \n",
      "4    1   11   0    7  10  \n",
      "5   11   30  26    0   4  \n",
      "6    0    6  13    0   0  \n",
      "7    4    8   7   14   2  \n",
      "8    2   24   4    0   1  \n",
      "9   15   48  34    3   9  \n",
      "10   6    8   0    0   0  \n",
      "11   5    3   1    6   2  \n",
      "12   0    2   0    0   3  \n",
      "13   3   38  23    5   3  \n",
      "14   3    1   3    0   2  \n",
      "15  36    9   2    0   0  \n",
      "16  10  632   6    5   0  \n",
      "17   1    6  85    0   4  \n",
      "18   0    4   1  251  41  \n",
      "19   0    2   3   34  79  \n"
     ]
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred_1 = model_1.predict(X_test)\n",
    "con_matrix=pd.DataFrame(confusion_matrix(y_true,y_pred_1))\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.60      0.45      0.51       118\n",
      "     british       0.51      0.40      0.45       181\n",
      "cajun_creole       0.67      0.61      0.64       349\n",
      "     chinese       0.72      0.76      0.74       609\n",
      "    filipino       0.56      0.49      0.52       174\n",
      "      french       0.49      0.51      0.50       596\n",
      "       greek       0.60      0.65      0.63       251\n",
      "      indian       0.83      0.83      0.83       693\n",
      "       irish       0.36      0.38      0.37       118\n",
      "     italian       0.78      0.80      0.79      1781\n",
      "    jamaican       0.77      0.58      0.66       128\n",
      "    japanese       0.66      0.66      0.66       311\n",
      "      korean       0.75      0.67      0.71       183\n",
      "     mexican       0.84      0.87      0.85      1435\n",
      "    moroccan       0.79      0.73      0.76       202\n",
      "     russian       0.34      0.34      0.34       106\n",
      " southern_us       0.67      0.68      0.68       931\n",
      "     spanish       0.36      0.37      0.37       227\n",
      "        thai       0.73      0.70      0.71       361\n",
      "  vietnamese       0.43      0.44      0.44       179\n",
      "\n",
      "    accuracy                           0.70      8933\n",
      "   macro avg       0.62      0.60      0.61      8933\n",
      "weighted avg       0.70      0.70      0.70      8933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_2 (solver='lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0   1    2    3   4    5    6    7   8     9   10   11   12    13   14  \\\n",
      "0   55   2    4    0   3    2    0    1   1     8   3    0    0    18    0   \n",
      "1    1  70    2    1   1   18    5    4  19    12   3    0    0     7    0   \n",
      "2    0   3  239    1   1    9    0    1   2    15   1    1    0    13    0   \n",
      "3    3   1    3  479  10    6    0    2   0     4   0   27   21     2    2   \n",
      "4    5   0    1   10  98    3    0    2   1     4   3   10    2    10    0   \n",
      "5    0   7   11    5   1  331    8    3   6   129   3    1    1    14    4   \n",
      "6    0   1    2    1   1    8  168    4   0    37   0    0    1     1    4   \n",
      "7    1   3    3    3   3    4    4  580   0     3   1   25    0    15   18   \n",
      "8    0  10    0    1   0   13    2    0  50     8   1    0    0     2    0   \n",
      "9    4   5   11    5   5  102   37    5   9  1487   2    2    0    27    7   \n",
      "10   5   1    2    2   3    0    0    6   0     3  80    3    0    11    0   \n",
      "11   3   0    3   32   4    5    1   17   0     9   0  211    8     3    0   \n",
      "12   0   0    0   18   0    1    0    0   0     5   0   20  129     2    0   \n",
      "13  11   3   15    8   4   12    3   14   6    29   1    2    0  1263    6   \n",
      "14   0   2    2    0   0    2    7   18   0     6   0    0    0     9  149   \n",
      "15   0   6    3    2   2   12    3    0   9    14   0    3    0     2    2   \n",
      "16   1  14   46   10   8   42    2    8  22    46  14    5    0    37    5   \n",
      "17   1   4    4    3   2   21    4    0   3    38   0    0    0    33    3   \n",
      "18   2   0    1   22   6    1    0   14   0     3   1    7    1     6    0   \n",
      "19   0   0    1   23   8    1    0    4   0     2   1    6    2     0    0   \n",
      "\n",
      "    15   16  17   18  19  \n",
      "0    1    3  12    5   0  \n",
      "1    3   28   6    0   1  \n",
      "2    3   57   3    0   0  \n",
      "3    1    7   2   23  16  \n",
      "4    0    7   0    6  12  \n",
      "5   13   35  23    0   1  \n",
      "6    2    7  13    1   0  \n",
      "7    5    8   6   10   1  \n",
      "8    5   23   2    0   1  \n",
      "9   10   32  25    3   3  \n",
      "10   1    9   2    0   0  \n",
      "11   4    5   2    3   1  \n",
      "12   0    1   0    2   5  \n",
      "13   4   38   9    4   3  \n",
      "14   1    0   4    1   1  \n",
      "15  41    6   1    0   0  \n",
      "16   8  648  12    2   1  \n",
      "17   1    9  99    0   2  \n",
      "18   0    1   2  256  38  \n",
      "19   1    3   3   32  92  \n"
     ]
    }
   ],
   "source": [
    "y_true = y_test\n",
    "y_pred_2 = model_2.predict(X_test)\n",
    "con_matrix=pd.DataFrame(confusion_matrix(y_true,y_pred_2))\n",
    "print(con_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "   brazilian       0.60      0.47      0.52       118\n",
      "     british       0.53      0.39      0.45       181\n",
      "cajun_creole       0.68      0.68      0.68       349\n",
      "     chinese       0.77      0.79      0.78       609\n",
      "    filipino       0.61      0.56      0.59       174\n",
      "      french       0.56      0.56      0.56       596\n",
      "       greek       0.69      0.67      0.68       251\n",
      "      indian       0.85      0.84      0.84       693\n",
      "       irish       0.39      0.42      0.41       118\n",
      "     italian       0.80      0.83      0.82      1781\n",
      "    jamaican       0.70      0.62      0.66       128\n",
      "    japanese       0.65      0.68      0.67       311\n",
      "      korean       0.78      0.70      0.74       183\n",
      "     mexican       0.86      0.88      0.87      1435\n",
      "    moroccan       0.74      0.74      0.74       202\n",
      "     russian       0.39      0.39      0.39       106\n",
      " southern_us       0.70      0.70      0.70       931\n",
      "     spanish       0.44      0.44      0.44       227\n",
      "        thai       0.74      0.71      0.72       361\n",
      "  vietnamese       0.52      0.51      0.52       179\n",
      "\n",
      "    accuracy                           0.73      8933\n",
      "   macro avg       0.65      0.63      0.64      8933\n",
      "weighted avg       0.73      0.73      0.73      8933\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Comparison & conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model_1(solver='adam')的準確率是0.70，model_2(solver='lbfgs')的準確率是0.73。 兩種模型的表現差不多。lbfgs適合用在小型數據集（收斂更快），但在此作業裡它對大型數據的處理也很好。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 助教們這學期辛苦了！ "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
